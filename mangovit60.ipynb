{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12734621,"sourceType":"datasetVersion","datasetId":8049534},{"sourceId":12747781,"sourceType":"datasetVersion","datasetId":8058549}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% 0. Imports & seed ------------------------------------------------------\nimport math, random, copy, collections, warnings\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, silhouette_score\nfrom sklearn.manifold import TSNE\nimport timm\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n\n# %% 1. Hyper-params you may tweak ------------------------------------------\nDATA_ROOT = Path(\"/kaggle/input/mangodata/Root/Original\")  # Updated path from image\nBATCH_PRE = 4          # Extremely small batch size for ViT (memory intensive)\nACC_STEPS = 32         # Very high accumulation steps to maintain effective batch size of 128\nPRE_EPOCHS = 100       # BYOL pre-training epochs\nBATCH_SUP = 2          # Extremely small supervised batch size for ViT\nEPOCHS_SUP = 100       # Supervised fine-tuning epochs\nLABELED_FRAC = 0.60    # % of train set with labels (70% labeled, 30% unlabeled)\nLINEAR_ONLY = False    # False = fine-tune whole encoder\nTEMP = 0.5             # NT-Xent temperature\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n\n# Check if data path exists\ntry:\n    assert DATA_ROOT.exists(), f\"❌ DATA_ROOT not found: {DATA_ROOT}\"\n    print(f\"✅ DATA_ROOT found: {DATA_ROOT}\")\nexcept AssertionError as e:\n    print(e)\n    # Fallback paths to try\n    fallback_paths = [\n        Path(\"/kaggle/input/mangodata/Root/Original\"),\n        Path(\"./data/Root/Original\"),\n        Path(\"./Root/Original\")\n    ]\n    for path in fallback_paths:\n        if path.exists():\n            DATA_ROOT = path\n            print(f\"✅ Using fallback path: {DATA_ROOT}\")\n            break\n    else:\n        raise FileNotFoundError(\"No valid data path found. Please check your data directory.\")\n\n# %% 2. Dataset root --------------------------------------------------------\nbase_ds = datasets.ImageFolder(DATA_ROOT, transform=None)\nprint(f\"{len(base_ds)} images, {len(base_ds.classes)} classes → {base_ds.classes}\")\n\n# %% 3. Improved ViT-optimized augmentations --------------------------------\n# Better normalization values for natural images\nmean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n\n# ViT-optimized augmentations for BYOL (ViT is more robust to strong augmentations)\nbyol_tf = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.2, 1.0), ratio=(0.75, 1.33)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.2),\n    transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2),  # Stronger for ViT\n    transforms.RandomGrayscale(p=0.2),\n    transforms.RandomApply([transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=0.5),\n    transforms.RandomApply([transforms.RandomRotation(degrees=45)], p=0.4),  # Stronger rotation for ViT\n    transforms.RandomApply([transforms.RandomPerspective(distortion_scale=0.3)], p=0.3),\n    transforms.RandomApply([transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))], p=0.3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n    transforms.RandomErasing(p=0.3, scale=(0.02, 0.4), ratio=(0.3, 3.3))  # Stronger erasing for ViT\n])\n\n# Supervised training augmentations\nsup_tf = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.1),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n    transforms.RandomRotation(degrees=15),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\ntest_tf = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\nclass BYOLDataset(Dataset):\n    def __init__(self, subset):\n        self.subset = subset\n        \n    def __len__(self):\n        return len(self.subset)\n    \n    def __getitem__(self, idx):\n        img, _ = self.subset[idx]\n        # Generate two different augmented views\n        view1 = byol_tf(img)\n        view2 = byol_tf(img)\n        return view1, view2\n\n# %% 4. Improved stratified splits -------------------------------------------\nindices = np.arange(len(base_ds))\nlabels = np.array([label for _, label in base_ds.imgs])\n\n# First split: train vs (val + test)\ntrain_idx, rest_idx = train_test_split(\n    indices, test_size=0.3, stratify=labels, random_state=SEED\n)\n\n# Second split: val vs test from the rest\nval_idx, test_idx = train_test_split(\n    rest_idx, test_size=0.5, stratify=labels[rest_idx], random_state=SEED\n)\n\nprint(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n\n# Create subsets\ntrain_base = torch.utils.data.Subset(base_ds, train_idx)\nval_base = torch.utils.data.Subset(base_ds, val_idx)\ntest_base = torch.utils.data.Subset(base_ds, test_idx)\n\n# Split training data into labeled and unlabeled portions\nperm = np.random.RandomState(SEED).permutation(len(train_base))\nn_labeled = max(len(base_ds.classes), int(LABELED_FRAC * len(train_base)))  # At least 1 per class\nlabeled_idx = perm[:n_labeled]\nunlabeled_idx = perm[n_labeled:]\n\nprint(f\"Labeled: {len(labeled_idx)}, Unlabeled: {len(unlabeled_idx)}\")\n\nclass TransformDataset(Dataset):\n    def __init__(self, subset, transform):\n        self.subset = subset\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.subset)\n    \n    def __getitem__(self, idx):\n        img, label = self.subset[idx]\n        return self.transform(img), label\n\n# Create final datasets\nlabeled_ds = TransformDataset(torch.utils.data.Subset(train_base, labeled_idx), sup_tf)\nval_ds = TransformDataset(val_base, test_tf)\ntest_ds = TransformDataset(test_base, test_tf)\nbyol_ds = BYOLDataset(base_ds)  # Use whole dataset for self-supervised learning\n\n# %% 5. DataLoaders ---------------------------------------------------------\npre_loader = DataLoader(byol_ds, batch_size=BATCH_PRE, shuffle=True, \n                       num_workers=0, pin_memory=False, drop_last=True)\nlab_loader = DataLoader(labeled_ds, batch_size=BATCH_SUP, shuffle=True,\n                       num_workers=0, pin_memory=False)\nval_loader = DataLoader(val_ds, batch_size=4, shuffle=False,  # Very small for ViT\n                       num_workers=0, pin_memory=False)\ntest_loader = DataLoader(test_ds, batch_size=4, shuffle=False,  # Very small for ViT\n                        num_workers=0, pin_memory=False)\n\nprint(f\"BYOL: {len(byol_ds)}, Labeled: {len(labeled_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n\n# %% 6. Vision Transformer BYOL model ---------------------------------------\ndef get_vit_encoder():\n    \"\"\"Create ViT encoder without pretrained weights - using smallest ViT variant\"\"\"\n    try:\n        # First try ViT-Tiny/16\n        model = timm.create_model('vit_tiny_patch16_224', pretrained=False, num_classes=0)\n        print(f\"✅ Created ViT-Tiny encoder with {sum(p.numel() for p in model.parameters()):,} parameters\")\n        return model\n    except Exception as e:\n        print(f\"❌ Failed to create ViT-Tiny: {e}\")\n        print(\"🔄 Falling back to MobileNetV3-Small...\")\n        # Fallback to a very lightweight CNN\n        model = timm.create_model('mobilenetv3_small_100', pretrained=False, num_classes=0)\n        print(f\"✅ Created MobileNetV3-Small encoder with {sum(p.numel() for p in model.parameters()):,} parameters\")\n        return model\n\n# Create a test encoder to determine the embedding dimension\ntest_encoder = get_vit_encoder()\nwith torch.no_grad():\n    test_input = torch.randn(1, 3, 224, 224)\n    test_output = test_encoder(test_input)\n    ENC_DIM = test_output.shape[1]\n    \nprint(f\"📏 Detected encoder embedding dimension: {ENC_DIM}\")\n\nPROJ_DIM = min(128, ENC_DIM // 2)    # Adaptive projection dimension\nPRED_DIM = min(128, ENC_DIM // 2)    # Adaptive prediction dimension\nEMA_DECAY = 0.996                    # Slightly higher EMA decay for ViT\n\n# Clean up test encoder\ndel test_encoder, test_input, test_output\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, use_bn=True, dropout=0.1):\n        super().__init__()\n        self.use_bn = use_bn\n        self.linear1 = nn.Linear(input_dim, hidden_dim)\n        if use_bn:\n            self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.gelu = nn.GELU()  # GELU activation for ViT\n        self.dropout = nn.Dropout(dropout) if dropout > 0 else None\n        self.linear2 = nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, x):\n        x = self.linear1(x)\n        if self.use_bn:\n            x = self.bn1(x)\n        x = self.gelu(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\nclass BYOL_ViT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Online network\n        self.online_encoder = get_vit_encoder()\n        self.online_projector = MLP(ENC_DIM, 512, PROJ_DIM, dropout=0.1)  # Much smaller hidden dim\n        self.online_predictor = MLP(PROJ_DIM, 512, PRED_DIM, dropout=0.1)  # Much smaller hidden dim\n        \n        # Target network (no gradients)\n        self.target_encoder = copy.deepcopy(self.online_encoder)\n        self.target_projector = copy.deepcopy(self.online_projector)\n        \n        # Disable gradients for target networks\n        for param in self.target_encoder.parameters():\n            param.requires_grad = False\n        for param in self.target_projector.parameters():\n            param.requires_grad = False\n    \n    @torch.no_grad()\n    def update_target_network(self):\n        \"\"\"Update target network with exponential moving average\"\"\"\n        for online_params, target_params in zip(self.online_encoder.parameters(), \n                                               self.target_encoder.parameters()):\n            target_params.data = EMA_DECAY * target_params.data + (1 - EMA_DECAY) * online_params.data\n            \n        for online_params, target_params in zip(self.online_projector.parameters(), \n                                               self.target_projector.parameters()):\n            target_params.data = EMA_DECAY * target_params.data + (1 - EMA_DECAY) * online_params.data\n    \n    def forward(self, view1, view2):\n        # Online network forward pass\n        online_repr1 = self.online_encoder(view1)\n        online_repr2 = self.online_encoder(view2)\n        \n        online_proj1 = self.online_projector(online_repr1)\n        online_proj2 = self.online_projector(online_repr2)\n        \n        online_pred1 = self.online_predictor(online_proj1)\n        online_pred2 = self.online_predictor(online_proj2)\n        \n        # Target network forward pass (no gradients)\n        with torch.no_grad():\n            target_repr1 = self.target_encoder(view1)\n            target_repr2 = self.target_encoder(view2)\n            \n            target_proj1 = self.target_projector(target_repr1)\n            target_proj2 = self.target_projector(target_repr2)\n        \n        return online_pred1, online_pred2, target_proj1.detach(), target_proj2.detach()\n\ndef byol_loss_fn(pred, target):\n    \"\"\"BYOL loss function with L2 normalization\"\"\"\n    pred = F.normalize(pred, dim=-1, p=2)\n    target = F.normalize(target, dim=-1, p=2)\n    return 2 - 2 * (pred * target).sum(dim=-1)\n\n# %% 7. BYOL pre-training with ViT ------------------------------------------\n# Clear any cached memory before starting training\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# Enable mixed precision training to reduce memory usage\nscaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\nuse_amp = torch.cuda.is_available()\n\nbyol_model = BYOL_ViT().to(DEVICE)\n\n# ViT-optimized optimizer with lower learning rate\noptimizer_pre = torch.optim.AdamW([\n    {'params': byol_model.online_encoder.parameters(), 'lr': 1e-4},\n    {'params': byol_model.online_projector.parameters(), 'lr': 1e-3},\n    {'params': byol_model.online_predictor.parameters(), 'lr': 1e-3}\n], weight_decay=1e-4)  # Higher weight decay for ViT\n\n# Learning rate scheduler with warmup (important for ViT)\ndef get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n    \n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\nwarmup_steps = PRE_EPOCHS // 5  # Longer warmup for ViT\nscheduler_pre = get_cosine_schedule_with_warmup(optimizer_pre, warmup_steps, PRE_EPOCHS)\n\nprint(f\"\\n▶ BYOL pre-training with ViT ({PRE_EPOCHS} epochs)\")\nbyol_losses = []\n\nfor epoch in range(1, PRE_EPOCHS + 1):\n    byol_model.train()\n    epoch_losses = []\n    \n    progress_bar = tqdm(pre_loader, desc=f\"Pre-train Epoch {epoch:03d}/{PRE_EPOCHS}\", leave=False)\n    \n    for step, (view1, view2) in enumerate(progress_bar):\n        view1, view2 = view1.to(DEVICE, non_blocking=True), view2.to(DEVICE, non_blocking=True)\n        \n        try:\n            # Forward pass with mixed precision\n            if use_amp:\n                with torch.cuda.amp.autocast():\n                    pred1, pred2, target1, target2 = byol_model(view1, view2)\n                    \n                    # Compute BYOL loss (symmetric)\n                    loss1 = byol_loss_fn(pred1, target2).mean()\n                    loss2 = byol_loss_fn(pred2, target1).mean()\n                    loss = loss1 + loss2\n                    loss = loss / ACC_STEPS  # Gradient accumulation\n                \n                # Backward pass with mixed precision\n                scaler.scale(loss).backward()\n            else:\n                # Forward pass\n                pred1, pred2, target1, target2 = byol_model(view1, view2)\n                \n                # Compute BYOL loss (symmetric)\n                loss1 = byol_loss_fn(pred1, target2).mean()\n                loss2 = byol_loss_fn(pred2, target1).mean()\n                loss = loss1 + loss2\n                loss = loss / ACC_STEPS  # Gradient accumulation\n                \n                # Backward pass\n                loss.backward()\n            \n            if (step + 1) % ACC_STEPS == 0:\n                if use_amp:\n                    # Gradient clipping with mixed precision\n                    scaler.unscale_(optimizer_pre)\n                    torch.nn.utils.clip_grad_norm_(byol_model.online_encoder.parameters(), 1.0)\n                    torch.nn.utils.clip_grad_norm_(byol_model.online_projector.parameters(), 1.0)\n                    torch.nn.utils.clip_grad_norm_(byol_model.online_predictor.parameters(), 1.0)\n                    \n                    scaler.step(optimizer_pre)\n                    scaler.update()\n                else:\n                    # Gradient clipping for stability (important for ViT)\n                    torch.nn.utils.clip_grad_norm_(byol_model.online_encoder.parameters(), 1.0)\n                    torch.nn.utils.clip_grad_norm_(byol_model.online_projector.parameters(), 1.0)\n                    torch.nn.utils.clip_grad_norm_(byol_model.online_predictor.parameters(), 1.0)\n                    \n                    optimizer_pre.step()\n                \n                optimizer_pre.zero_grad()\n                byol_model.update_target_network()\n            \n            epoch_losses.append(loss.item() * ACC_STEPS)\n            progress_bar.set_postfix({'loss': f'{loss.item() * ACC_STEPS:.4f}'})\n            \n            # Clear intermediate tensors to save memory\n            del pred1, pred2, target1, target2, loss1, loss2, loss\n            \n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"Warning: OOM at step {step}, clearing cache and skipping batch\")\n                torch.cuda.empty_cache()\n                optimizer_pre.zero_grad()\n                continue\n            else:\n                raise e\n        \n        # Periodic memory cleanup\n        if step % 50 == 0 and torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    # Step scheduler\n    scheduler_pre.step()\n    \n    avg_loss = np.mean(epoch_losses)\n    byol_losses.append(avg_loss)\n    \n    if epoch % 10 == 0 or epoch == 1:\n        print(f\"Epoch {epoch:03d}: BYOL loss = {avg_loss:.4f}, LR = {scheduler_pre.get_last_lr()[0]:.6f}\")\n\n# Save encoder weights\nencoder_checkpoint = copy.deepcopy(byol_model.online_encoder).cpu().state_dict()\n\n# Plot pre-training loss\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, PRE_EPOCHS + 1), byol_losses, 'b-', linewidth=2)\nplt.title('BYOL Pre-training Loss (ViT Backbone)')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# %% 7.5. t-SNE Visualization and Silhouette Score Analysis ----------------\nprint(\"\\n▶ Performing t-SNE visualization and silhouette analysis...\")\n\ndef extract_features_for_analysis(model, data_loader, max_samples=100):  # Much smaller sample size\n    \"\"\"Extract features from the encoder for analysis\"\"\"\n    model.eval()\n    features = []\n    labels_list = []\n    \n    with torch.no_grad():\n        for i, (images, labels) in enumerate(data_loader):\n            if len(features) >= max_samples:\n                break\n            \n            # Process one image at a time to avoid OOM\n            for j in range(images.size(0)):\n                if len(features) >= max_samples:\n                    break\n                \n                single_img = images[j:j+1].to(DEVICE, non_blocking=True)\n                try:\n                    feat = model(single_img)\n                    features.append(feat.cpu().numpy())\n                    labels_list.append(labels[j].item())\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n                except RuntimeError as e:\n                    if \"out of memory\" in str(e):\n                        print(f\"OOM during feature extraction, skipping sample\")\n                        torch.cuda.empty_cache()\n                        continue\n                    else:\n                        raise e\n    \n    if features:\n        return np.vstack(features)[:max_samples], np.array(labels_list)[:max_samples]\n    else:\n        return np.array([]).reshape(0, ENC_DIM), np.array([])\n\n# Extract features using the pre-trained ViT encoder\nencoder_for_analysis = get_vit_encoder()\nencoder_for_analysis.load_state_dict(encoder_checkpoint)\nencoder_for_analysis.to(DEVICE)\n\n# Create a sample of test data for analysis\nanalysis_features, analysis_labels = extract_features_for_analysis(encoder_for_analysis, test_loader, max_samples=100)\n\n# Compute silhouette score\nprint(\"Computing silhouette score...\")\nif len(np.unique(analysis_labels)) > 1:\n    silhouette_avg = silhouette_score(analysis_features, analysis_labels)\n    print(f\"🎯 Silhouette Score (ViT Pre-training): {silhouette_avg:.4f}\")\n    \n    # Per-class silhouette scores\n    from sklearn.metrics import silhouette_samples\n    sample_silhouette_values = silhouette_samples(analysis_features, analysis_labels)\n    \n    print(\"\\nPer-class silhouette scores:\")\n    for i in range(len(base_ds.classes)):\n        class_silhouette = sample_silhouette_values[analysis_labels == i]\n        if len(class_silhouette) > 0:\n            print(f\"  {base_ds.classes[i]}: {np.mean(class_silhouette):.4f}\")\n\n# Perform t-SNE visualization\nprint(\"Computing t-SNE embedding...\")\ntsne = TSNE(n_components=2, random_state=SEED, perplexity=30, n_iter=1000)\ntsne_results = tsne.fit_transform(analysis_features)\n\n# Plot t-SNE results\nplt.figure(figsize=(12, 8))\ncolors = plt.cm.Set3(np.linspace(0, 1, len(base_ds.classes)))\n\nfor i, class_name in enumerate(base_ds.classes):\n    mask = analysis_labels == i\n    if np.any(mask):\n        plt.scatter(tsne_results[mask, 0], tsne_results[mask, 1], \n                   c=[colors[i]], label=class_name, alpha=0.7, s=50)\n\nplt.title('t-SNE Visualization of ViT Pre-trained Features')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# %% 8. Enhanced Supervised fine-tuning with ViT ---------------------------\n# Initialize ViT encoder and classifier\nencoder = get_vit_encoder().to(DEVICE)\nencoder.load_state_dict(encoder_checkpoint)\n\nif LINEAR_ONLY:\n    for param in encoder.parameters():\n        param.requires_grad = False\n    print(\"Fine-tuning: Linear probe only (ViT)\")\nelse:\n    print(\"Fine-tuning: Full ViT model\")\n\n# Enhanced classifier with dropout\nclassifier = nn.Sequential(\n    nn.Dropout(0.3),  # Lower dropout for ViT\n    nn.Linear(ENC_DIM, len(base_ds.classes))\n).to(DEVICE)\n\n# Initialize classifier with Xavier uniform\nnn.init.xavier_uniform_(classifier[1].weight)\nnn.init.constant_(classifier[1].bias, 0)\n\n# ViT-optimized optimizer with different learning rates\nif LINEAR_ONLY:\n    optimizer_sup = torch.optim.AdamW(classifier.parameters(), lr=1e-3, weight_decay=1e-4)\nelse:\n    optimizer_sup = torch.optim.AdamW([\n        {'params': encoder.parameters(), 'lr': 1e-5},  # Very low LR for ViT encoder\n        {'params': classifier.parameters(), 'lr': 1e-3}  # Higher LR for classifier\n    ], weight_decay=1e-4)\n\n# Enhanced scheduler\nscheduler_sup = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer_sup, mode='max', factor=0.5, patience=15, verbose=True\n)\n\n# Loss function with label smoothing\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n# Training history\ntrain_losses, val_losses, val_accuracies = [], [], []\nbest_val_acc = 0.0\n\nprint(f\"\\n▶ Enhanced Fine-tuning with ViT ({EPOCHS_SUP} epochs)\")\n\nfor epoch in range(1, EPOCHS_SUP + 1):\n    # Training phase\n    encoder.train()\n    classifier.train()\n    train_loss_batch = []\n    \n    progress_bar = tqdm(lab_loader, desc=f\"Fine-tune Epoch {epoch:03d}/{EPOCHS_SUP}\", leave=False)\n    \n    for images, labels in progress_bar:\n        images, labels = images.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n        \n        try:\n            # Forward pass\n            features = encoder(images)\n            logits = classifier(features)\n            loss = criterion(logits, labels)\n            \n            # Backward pass\n            optimizer_sup.zero_grad()\n            loss.backward()\n            \n            # Gradient clipping (important for ViT)\n            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0)\n            torch.nn.utils.clip_grad_norm_(classifier.parameters(), 1.0)\n            \n            optimizer_sup.step()\n            \n            train_loss_batch.append(loss.item())\n            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n            \n            # Clear intermediate tensors\n            del features, logits, loss\n            \n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"Warning: OOM during training, clearing cache and skipping batch\")\n                torch.cuda.empty_cache()\n                optimizer_sup.zero_grad()\n                continue\n            else:\n                raise e\n    \n    # Validation phase\n    encoder.eval()\n    classifier.eval()\n    val_loss_batch = []\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n            \n            try:\n                features = encoder(images)\n                logits = classifier(features)\n                loss = criterion(logits, labels)\n                \n                val_loss_batch.append(loss.item())\n                \n                _, predicted = torch.max(logits.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n                \n                # Clear intermediate tensors\n                del features, logits, loss\n                \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    print(f\"Warning: OOM during validation, clearing cache and skipping batch\")\n                    torch.cuda.empty_cache()\n                    continue\n                else:\n                    raise e\n    \n    # Periodic memory cleanup\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Calculate metrics\n    avg_train_loss = np.mean(train_loss_batch)\n    avg_val_loss = np.mean(val_loss_batch)\n    val_acc = 100.0 * correct / total\n    \n    train_losses.append(avg_train_loss)\n    val_losses.append(avg_val_loss)\n    val_accuracies.append(val_acc)\n    \n    # Step scheduler\n    scheduler_sup.step(val_acc)\n    \n    # Track best validation accuracy\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        # Save best model\n        torch.save({\n            'encoder': encoder.state_dict(),\n            'classifier': classifier.state_dict(),\n            'val_acc': val_acc\n        }, 'best_vit_model.pth')\n    \n    if epoch % 10 == 0 or epoch == 1:\n        print(f\"Epoch {epoch:03d}: Train Loss = {avg_train_loss:.4f}, \"\n              f\"Val Loss = {avg_val_loss:.4f}, Val Acc = {val_acc:.2f}%, \"\n              f\"Best Val Acc = {best_val_acc:.2f}%\")\n\n# Load best model\ntry:\n    checkpoint = torch.load('best_vit_model.pth')\n    encoder.load_state_dict(checkpoint['encoder'])\n    classifier.load_state_dict(checkpoint['classifier'])\n    print(f\"Loaded best ViT model with validation accuracy: {checkpoint['val_acc']:.2f}%\")\nexcept:\n    print(\"Using final ViT model (best model checkpoint not found)\")\n\n# Plot training curves\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\nax1.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss', linewidth=2)\nax1.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Val Loss', linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Training and Validation Loss (ViT)')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(range(1, len(val_accuracies) + 1), val_accuracies, 'g-', label='Val Accuracy', linewidth=2)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.set_title('Validation Accuracy (ViT)')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# %% 8.5. Post-training t-SNE and Silhouette Analysis ----------------------\nprint(\"\\n▶ Post-training ViT feature analysis...\")\n\n# Extract features from fine-tuned ViT encoder\npost_features, post_labels = extract_features_for_analysis(encoder, test_loader, max_samples=100)\n\n# Compute post-training silhouette score\nif len(np.unique(post_labels)) > 1:\n    post_silhouette_avg = silhouette_score(post_features, post_labels)\n    print(f\"🎯 Silhouette Score (ViT Post-training): {post_silhouette_avg:.4f}\")\n    print(f\"📈 Silhouette Improvement: {post_silhouette_avg - silhouette_avg:.4f}\")\n\n# Post-training t-SNE\nprint(\"Computing post-training t-SNE embedding...\")\npost_tsne = TSNE(n_components=2, random_state=SEED, perplexity=30, n_iter=1000)\npost_tsne_results = post_tsne.fit_transform(post_features)\n\n# Plot comparison of pre-training vs post-training t-SNE\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n\n# Pre-training t-SNE\nfor i, class_name in enumerate(base_ds.classes):\n    mask = analysis_labels == i\n    if np.any(mask):\n        ax1.scatter(tsne_results[mask, 0], tsne_results[mask, 1], \n                   c=[colors[i]], label=class_name, alpha=0.7, s=50)\n\nax1.set_title('t-SNE: ViT Pre-training Features')\nax1.set_xlabel('t-SNE Component 1')\nax1.set_ylabel('t-SNE Component 2')\nax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nax1.grid(True, alpha=0.3)\n\n# Post-training t-SNE\nfor i, class_name in enumerate(base_ds.classes):\n    mask = post_labels == i\n    if np.any(mask):\n        ax2.scatter(post_tsne_results[mask, 0], post_tsne_results[mask, 1], \n                   c=[colors[i]], label=class_name, alpha=0.7, s=50)\n\nax2.set_title('t-SNE: ViT Post-training Features')\nax2.set_xlabel('t-SNE Component 1')\nax2.set_ylabel('t-SNE Component 2')\nax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# %% 9. Final evaluation ----------------------------------------------------\nencoder.eval()\nclassifier.eval()\n\ny_true, y_pred, y_probs = [], [], []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n        \n        try:\n            features = encoder(images)\n            logits = classifier(features)\n            probs = F.softmax(logits, dim=1)\n            \n            y_true.extend(labels.cpu().tolist())\n            y_pred.extend(torch.argmax(logits, dim=1).cpu().tolist())\n            y_probs.extend(probs.cpu().numpy())\n            \n            # Clear intermediate tensors\n            del features, logits, probs\n            \n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"Warning: OOM during evaluation, clearing cache and skipping batch\")\n                torch.cuda.empty_cache()\n                continue\n            else:\n                raise e\n\n# Calculate test accuracy\ntest_accuracy = 100.0 * sum(np.array(y_true) == np.array(y_pred)) / len(y_true)\nprint(f\"\\n🎯 Final Test Accuracy: {test_accuracy:.2f}%\")\n\n# Classification report\nprint(\"\\n📊 Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=base_ds.classes, digits=3))\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=base_ds.classes, yticklabels=base_ds.classes)\nplt.title(f'Confusion Matrix - ViT (Test Accuracy: {test_accuracy:.2f}%)')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n# %% 10. Summary of Results -------------------------------------------------\nprint(\"\\n\" + \"=\"*70)\nprint(\"📋 FINAL RESULTS SUMMARY (Memory-Optimized Backbone)\")\nprint(\"=\"*70)\nprint(f\"🎯 Final Test Accuracy: {test_accuracy:.2f}%\")\nprint(f\"🎯 Best Validation Accuracy: {best_val_acc:.2f}%\")\nif 'silhouette_avg' in locals() and len(analysis_features) > 0:\n    print(f\"📊 Pre-training Silhouette Score: {silhouette_avg:.4f}\")\nif 'post_silhouette_avg' in locals() and len(post_features) > 0:\n    print(f\"📊 Post-training Silhouette Score: {post_silhouette_avg:.4f}\")\n    print(f\"📈 Silhouette Improvement: {post_silhouette_avg - silhouette_avg:.4f}\")\n\nprint(f\"\\n📈 Training Configuration:\")\nprint(f\"   - Backbone: Auto-detected ({ENC_DIM}-dim embeddings)\")\nprint(f\"   - Pre-training epochs: {PRE_EPOCHS}\")\nprint(f\"   - Fine-tuning epochs: {EPOCHS_SUP}\")\nprint(f\"   - Labeled fraction: {LABELED_FRAC:.1%}\")\nprint(f\"   - Linear probe only: {LINEAR_ONLY}\")\nprint(f\"   - EMA decay: {EMA_DECAY}\")\nprint(f\"   - Batch sizes: Pre-train={BATCH_PRE}, Fine-tune={BATCH_SUP}\")\nprint(f\"   - Gradient accumulation steps: {ACC_STEPS}\")\nprint(f\"   - Mixed precision training: {use_amp}\")\n\nprint(f\"\\n✅ Memory-optimized training completed successfully!\")\nprint(\"=\"*70)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T21:36:40.046951Z","iopub.execute_input":"2025-08-17T21:36:40.047191Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n✅ DATA_ROOT found: /kaggle/input/mangodata/Root/Original\n837 images, 6 classes → ['Amrapali Mango', 'Banana Mango', 'Chaunsa Mango', 'Fazli Mango', 'Haribhanga Mango', 'Himsagar Mango']\nTrain: 585, Val: 126, Test: 126\nLabeled: 351, Unlabeled: 234\nBYOL: 837, Labeled: 351, Val: 126, Test: 126\n✅ Created ViT-Tiny encoder with 5,524,416 parameters\n📏 Detected encoder embedding dimension: 192\n✅ Created ViT-Tiny encoder with 5,524,416 parameters\n\n▶ BYOL pre-training with ViT (100 epochs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 001/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 001: BYOL loss = 4.0276, LR = 0.000005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 002/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 003/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 004/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 005/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 006/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 007/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 008/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 009/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 010/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 010: BYOL loss = 2.1816, LR = 0.000050\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 011/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 012/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 013/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 014/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 015/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 016/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 017/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 018/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 019/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 020/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 020: BYOL loss = 2.3932, LR = 0.000100\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 021/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 022/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 023/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 024/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 025/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 026/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 027/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 028/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 029/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 030/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 030: BYOL loss = 2.3990, LR = 0.000096\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 031/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 032/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 033/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 034/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 035/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 036/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 037/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 038/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 039/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 040/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 040: BYOL loss = 2.2838, LR = 0.000085\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 041/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 042/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 043/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 044/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 045/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 046/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 047/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 048/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 049/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 050/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 050: BYOL loss = 2.1136, LR = 0.000069\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 051/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 052/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 053/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 054/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 055/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 056/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 057/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 058/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 059/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 060/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 060: BYOL loss = 1.9618, LR = 0.000050\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 061/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 062/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 063/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 064/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 065/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 066/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 067/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 068/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 069/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 070/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 070: BYOL loss = 1.8822, LR = 0.000031\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 071/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 072/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 073/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 074/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Pre-train Epoch 075/100:   0%|          | 0/209 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"493275963cbc4f6ab4acf7df2b12e79c"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}